---
title: "Practical 2"
author: "Matteo Larrode"
date: "2023-10-18"
output: html_document
---

# Week 2: Causation and Randomised Experiments

### Question 1: A demonstration of the logic behind randomised experiments

We will demonstrate that randomised experiments work by creating an imaginary experiment. 

```{r setup}
library(tidyverse)
load("data/experiment.Rda")
```

This dataset includes the potential outcome under control (y0) and the potential outcome under treatment (y1) for 100 units that form the sample for our experiment.

*This is a purely hypothetical scenario: In reality, we never observe potential outcomes under both treatment and control for the same units: we only observe one of them*

a) Find the *true Average Treatment Effect* for all units, using y0 and y1

```{r}
mean(a$y1-a$y0)
```

b) Now, we’ll randomly assign half of the units to treatment and half to control by creating a new variable indicating treatment status. 

```{r}
set.seed(1) 

a$rand <- sample(c(1:100))
a <- a[order(a$rand),]
a$tr <- c(rep(1,50),rep(0,50))
```

c) Conduct a test to assess whether the treatment and control groups have the same average potential outcomes under control (y0). Has randomisation succeeded in creating treatment and control groups with equivalent potential outcomes under control?

```{r}
t.test(a$y0[a$tr==1], a$y0[a$tr==0])
```
There is a small, but not statistically significant, difference in potential outcomes under control of 2.26 between the two groups (the p-value is 0.11). Thus, as expected, we cannot reject the null hypothesis that the potential outcomes under control are equal.

_Why isn't the difference precisely equal to zero?_: randomisation isn’t guaranteed to completely equalise potential outcomes in any one instance. Instead, it does so in expectation over many repeated randomisations, as we show below.


d) Find the *Average Treatment Effect from the experiment*. How similar is it to the true Average Treatment Effect?
```{r}
 mean(a$y1[a$tr==1]) - mean(a$y0[a$tr==0])
```
The estimated ATE is close to the true ATE but not exactly the same. Any one randomisation isn’t guaranteed to give the true ATE.

#### e) Performance of experimental procedure over repeated randomisations, using a simulation ####
```{r}
experiment.sim <- function(a){
  a$rand <- sample(c(1:100))
  a <- a[order(a$rand),]
  a$tr <- c(rep(1,50),rep(0,50))
  
  mean(a$y1[a$tr==1]) - mean(a$y0[a$tr==0])
  }

sims <- replicate(10000,experiment.sim(a))

mean(sims)
```
The mean ATE is now extremely close to the true ATE. This shows that randomised experiments work! On average across repeated randomisations, we obtain the true ATE. 

In any one instance, the estimate will not be exactly the same, but the estimator is unbiased because it recovers the true ATE in expectation, if we kept randomising over and over again.

f) Finally, repeat (e), calculating the mean difference in potential outcomes under control (y0) between the treatment and control groups instead of the ATE. What is the mean difference from your 10,000 experiments?

```{r}
experiment.sim2 <- function(a){
  a$rand <- sample(c(1:100))
  a <- a[order(a$rand),]
  a$tr <- c(rep(1,50),rep(0,50))
  mean(a$y0[a$tr==1] - a$y0[a$tr==0])
  }

sims2 <- replicate(10000,experiment.sim2(a))

mean(sims2)
```
The mean difference is now extremely close to zero. This demonstrates that on average, experiments remove selection bias. 

Again, in any one instance the treatment and control groups will not be exactly alike, but across repeated randomisations, their potential outcomes are close to identical.

### Question 2
